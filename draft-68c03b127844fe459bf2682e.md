---
title: "TILED GEMM"
slug: tiled-gemm

---

## Recap: Naive GEMM Limitations

In part-2 of our blog series on naive GEMM implementation, we found that even with optimal memory access patterns‚Äîbroadcast reads for Matrix A and coalesced reads for Matrix B‚Äîthe performance was significantly constrained, reaching only about 2% of the theoretical peak memory bandwidth.

The primary issue was inadequate data reuse due to cache capacity limitations. Although our (32,32) block configuration facilitated efficient warp-level memory patterns, frequent cache evictions led to repeated global memory accesses for the same data elements. This resulted in a memory-bound kernel that was unable to fully leverage the GPU‚Äôs computational resources.

## Moving Forward: Shared Memory as the Solution

To overcome these limitations of naive GEMM, we need explicit control over data locality and reuse. The L1 cache, being a hardware-managed cache controlled by the execution framework, provides no guarantee that data brought into cache will remain available for subsequent transactions.

Shared memory, managed by the programmer and often called a ‚Äúsoftware cache,‚Äù provides a solution by allowing programmers to control data movement explicitly. Once data is loaded into shared memory, it is guaranteed to remain available until either overwritten by the program or the kernel execution completes.

However, it is not possible to bring all matrix data into shared memory simultaneously. To understand why, let us examine the key limitations that prevent this approach:

## CUDA Programming Constraints - Ampere Architecture

Understanding memory and thread limitations when developing CUDA applications for matrix operations.

<aside>
  <div>
    Shared Memory Constraints
  </div>
  <div>
    <h3>Memory Allocation Limits</h3>
    <p>
      Shared memory is configurable, with a maximum allocation of 
      <b>164 KB per thread block</b> 
      (from the total 192 KB block)
    </p>
  </div>
  <div>
    <h3>Example Matrix Memory Requirements</h3>
    <p>Our example matrix A (256x256) requires:</p>
    <div>
      <code>256 √ó 256 √ó 4 bytes = 256 KB of memory</code>
    </div>
  </div>
 <br />
  <div>
    <h3>Access Restrictions</h3>
    <p>
      Data in shared memory is accessible only to threads within the same thread block
    </p>
  </div>
  </aside>

<div data-node-type="callout">
<div data-node-type="callout-emoji">üí°</div>
<div data-node-type="callout-text">The matrix requires 256 KB but only 164 KB is available per block, creating a significant constraint for large matrix operations.</div>
</div>

<aside>
  <div>
    Thread Block Limitations
  </div>
  <div>
    <h3>Total Threads Required</h3>
    <p>
      To calculate the complete output matrix C (256√ó256 = 
      <b>65,536 elements</b>), 
      we would need 65,536 threads
    </p>
  </div>
  <div>
    <h3>Ampere Architecture Limits</h3>
    <p>
      Maximum threads per thread block in Ampere architecture: 
      <b>1,024 threads</b>
    </p>
  </div>
  <div>
    <h3>Thread Capacity Analysis</h3>
    <p>
      This represents only approximately <b>1.6%</b> of the required threads:
    </p>
    <div>
      <code>1,024 √∑ 65,536 = 0.0156 (1.56%)</code>
    </div>
  </div>
</aside>

<div data-node-type="callout">
<div data-node-type="callout-emoji">‚ö°</div>
<div data-node-type="callout-text">The huge gap between required threads (65,536) and available threads per block (1,024) necessitates careful work distribution across multiple thread blocks.</div>
</div>

## Tiling Strategy Options for 256√ó256 Matrices

These constraints reveal that while matrix A cannot fit entirely into one thread block‚Äôs shared memory, it can be split into smaller sub-matrices (tiles) that do fit. This approach forms the foundation of Tiled GEMM, where matrices are divided into manageable tiles that individual thread blocks can process effectively using shared memory. The tiling strategy is designed from the output matrix‚Äôs perspective.

## Tiling Strategy Options

Several tiling strategies can be used to partition the 256√ó256 matrices into smaller sub-matrices. The tile size significantly impacts both memory usage and thread utilization, so it must be chosen carefully to achieve optimal performance. from the below listed we can‚Äôt proceed with some of the options due to limitations we discussed earlier like number of threads per block , ex:- 64 *64 tile needs 4096 threads per block , but the maximum number of threads per block configuration is 1024, hence we can eliminate the Tile sizes 128*128 & 64*64, for our example we will consider tile size of 32*32.

### Tile Size

Matrix Size: 256 √ó 256 = 65,536 elements For tile size T√óT: Number of tiles = (256/T)¬≤

* 128√ó128 tiles: (256/128)¬≤ = 2¬≤ = 4 tiles
    
* 64√ó64 tiles: (256/64)¬≤ = 4¬≤ = 16 tiles
    
* 32√ó32 tiles: (256/32)¬≤ = 8¬≤ = 64 tiles
    
* 16√ó16 tiles: (256/16)¬≤ = 16¬≤ = 256 tiles
    
* 8√ó8 tiles: (256/8)¬≤ = 32¬≤ = 1,024 tiles
    
* 4√ó4 tiles: (256/4)¬≤ = 6
    

| Tile Size | Elements per Tile | Memory/Tile | Threads Required |
| --- | --- | --- | --- |
| 128 √ó 128 | 16384 (128 √ó 128) | 64 KB | 16384 |
| 64 √ó 64 | 4096 ( 64 √ó 64) | 16 KB | 4096 |
| 32 √ó 32 | 1024 (32 √ó 32) | 4 KB | 1024 |
| 16 √ó 16 | 256 (16 √ó 16) | 1 KB | 256 |
| 8 √ó 8 | 64 (8 √ó 8) | 256 B | 64 |
| 4 √ó 4 | 16 (4 √ó 4) | 64 B | 16 |

# Tiling Step by Step

> For our example of **256√ó256 matrix**, we are splitting the large matrix into tiles (sub-matrices) of size **32√ó32**, this leads to the following configuration:

1. **Total tiles**
    
    `64 tiles (8√ó8 grid), with each tile calculated by one thread block`
    
2. **Tile size**
    
    `Each thread block computes one 32√ó32 tile, totaling 1,024 elements`
    
3. **Thread allocation**
    
    `Each thread block uses 1,024 threads (one thread per tile element)`
    
4. **Warp organization**
    
    `Each thread block contains 32 warps 1,024 √∑ 32 = 32 warps`
    
5. **Memory footprint**
    
    `Each thread block loads 32√ó32 elements = 4 KB per matrix (float32 precision)`
    
6. **Dependencies**
    
    `To calculate one tile C(0,0), it requires 8 tiles from matrix A (row 0) and 8 tiles from matrix B (column 0)`
    
7. **Loading strategy**
    
    `Tiles from A and B are cooperatively loaded by warps into the thread block's shared memory`
    
8. **Sequential processing**
    
    `Tiles of A and B are loaded in 8 sequential phases as described below`
    

<div data-node-type="callout">
<div data-node-type="callout-emoji">üí°</div>
<div data-node-type="callout-text">The tiling approach enables efficient use of shared memory by loading small, manageable chunks of data that fit within memory constraints while maximizing thread utilization and computational efficiency.</div>
</div>

We‚Äôll examine how Thread Block (0,0) processes these tiles through the 8 sequential phases to compute C(0,0) using shared memory optimization.

**Full Grid** : This visualization represents the computation of a single tile C(0,0). Simultaneously, the remaining 63 thread blocks execute identical processes to compute tiles assigned to the block,The collective output of all 64 thread blocks C(0,0) to C(7,7) yields the final 256√ó256 matrix containing 65,536 elements.

**Grid Layout(Tiles)**

<iframe src="https://project-hwtve.vercel.app/tile_gemm_visulization/index.html" width="130%" height="1200px" style="border:none;zoom:0.8;max-width:130%;display:block;margin:0">
</iframe>

![tiled_matrix_multiplication](http://localhost:1313/images/cuda/tile_gemm/tiled_matrix_multiplication.png align="left")

**Tile C(0,0)**

![tile_c00](http://localhost:1313/images/cuda/tile_gemm/tile-c00-generation.png align="left")

# Data Reuse Analysis: Naive GEMM vs Tile GEMM

Let us examine how tiling solves the data reuse inefficiencies that arise in naive GEMM cache evictions.

Configuration Recap:

**Naive GEMM:** Grid of 1,024 thread blocks (32√ó32 grid), each thread block containing 1,024 threads (32√ó32), with no shared memory usage.

**Tiled GEMM:** Grid of 64 thread blocks (8√ó8 grid), each thread block containing 1,024 threads (32√ó32), using 32√ó32 tiles with shared memory for matrices A and B.

If we revisit the configuration and execution for Naive GEMM , our configuration comprised Grid of 64 Thread Blocks in 2D (32,32) & Thread Block comprised of 1024 threads in 2D (32,32) , each Thread block had 8 Warps, each warp calculating 32 elements in output matrix.

## Data Reuse Analysis

**Naive GEMM Limitations:**

To calculate adjacent output elements C(0,0) and C(0,1), both require the entire first row of matrix A. While this row is loaded once from global memory for C(0,0), cache evictions prevent reuse for C(0,1), forcing redundant global memory accesses.

In order to calculate C(0,0) and C(1,0) we need the first column of B matrix i.e B(0,0) =&gt; B(0,31), while calculating C(0,0) this whole column had been fetched from Global memory and used for C(0,0) the same could not be reused for performing computations for C(1,0) due to cache evictions, we will briefly look at the steps in Warp-0

### Memory Access per Thread Block (Naive):

Load Row-0 from Matrix A: 256 √ó 4 bytes = 1 KB Load Columns 0-31 from Matrix B: 256 √ó 32 √ó 4 bytes = 32 KB

#### Data Loading from Global Memory:

* Matrix A: 1 row = 256 elements √ó 4 bytes = 1,024 bytes
    
* Matrix B: 1 column = 256 elements √ó 4 bytes = 1,024 bytes
    
* Total: 2,048 bytes
    

#### Operations Performed:

256 multiply operations + 256 add operations = 512 FLOPs

```markdown
Arithmetic Intensity:
512 FLOPs √∑ 2,048 bytes = 0.25 FLOPS/Byte
```

> **Problem: Matrix B columns cannot be reused across computations due to cache evictions**

### Tiled GEMM Solution:

#### Data Loading from Global Memory:

* Load 32√ó32 tile from Matrix A into shared memory: 32 √ó 32 √ó 4 bytes \* 8 Tiles = 32 KB
    
* Load 32√ó32 tile from Matrix B into shared memory: 32 √ó 32 √ó 4 bytes \* 8 Tiles = 32 KB
    
* Total: 64 KB = 65,536 bytes
    

#### Operations Performed:

32√ó32 threads √ó 512 operations each = 524,288 FLOPs

```markdown
Arithmetic Intensity:
524,288FLOPs √∑ 65,536 bytes = 8.0 FLOPS/Byte
```

<div data-node-type="callout">
<div data-node-type="callout-emoji">üí°</div>
<div data-node-type="callout-text">The improvement comes from data reuse within shared memory. In tiled GEMM, each byte loaded from global memory is reused multiple times across different computations, while in naive GEMM, each byte is used only once before being potentially evicted from cache.</div>
</div>

**Below illustration explain the steps that lead to Naive GEMM Cache Evictions**

![naiv_gemm_cache_evictions](http://localhost:1313/images/cuda/tile_gemm/naive_gemm_cache_eviction_issues.png align="left")

![tile_gemm_reuse](http://localhost:1313/images/cuda/tile_gemm/tile_gemm_shared_memory_reuse.png align="left")

# Performance Analysis

To understand optimizations achieved by Tiled GEMM, we need to understand the theoritical limits of Hardware to calculate Arthimatic Inensity for both Naive & Tiled GEMM

## A100 Specifications

* CUDA Cores: 6,912 (108 SMs √ó 64 cores per SM)
    
* Base Clock: ~1.41 GHz
    
* Memory: 80GB HBM2e
    
* Memory Interface: 5,120-bit bus width
    
* Memory Clock: ~1.6 GHz (effective)
    

## Peak FLOPS Calculation

* Cores per SM: 64 CUDA cores
    
* Total SMs: 108 streaming multiprocessors
    
* Total CUDA cores: 108 √ó 64 = 6,912 cores
    
* Base clock frequency: ~1.41 GHz
    
* Operations per core per clock: 1 FMA = 2 FLOPs
    

## Peak FP32 Performance:

Peak FLOPS = Total Cores √ó Clock Frequency √ó FLOPs per Clock  
Peak FLOPS = 6,912 √ó 1.41 √ó 10‚Åπ √ó 2  
Peak FLOPS ‚âà 19.5 TFLOPS

## Peak Memory Bandwidth Calculation

Memory interface width: 5,120 bits = 640 bytes  
Memory clock (effective): ~1,600 MHz (DDR, so 800 MHz √ó 2)

## Peak Memory Bandwidth:

Peak Bandwidth = Interface Width √ó Memory Clock  
Peak Bandwidth = 640 bytes √ó 1,600 √ó 10‚Å∂ transfers/second  
Peak Bandwidth ‚âà 2,039 GB/s ‚âà 2.0 TB/s

## Peak Arithmetic Intensity

Arithmetic Intensity = FLOPS √∑ Memory Bandwidth  
19.5 *10^12 FLOPS / 2.0* 10^12 Bytes per sec  
19.5/2 = 9.75 Flops/Byte

## Arithemetic Intensity :

To calculate each cell in the output matrix we need to fetch 256 elements from A & 256 Elements from B , and perform 256 Multiply and 256 Additions

**Naive GEMM**

```markdown
C(0,0) = A(0,0) * B(0,0) + A(0,1) * B(1,0) + A(0,2) * B(2,0) + ... + A(0,256) * B(256,0)

FLOPS = 256 Multiply  + 256 Additions = 512 FLOPS
Bytes Transferred = 256 * 4 Bytes (A) + 256 * 4 Byes (B) = 2 KB = 2048 Bytes

FLOPS/Bytes = 512 / 2048 = 0.25 FLOPS/Byte
```

<div data-node-type="callout">
<div data-node-type="callout-emoji">üí°</div>
<div data-node-type="callout-text">To calculate each cell in the output matrix we need to load 8 Tiles each from A &amp; B over several phases, each phase includes 32 Multiply &amp; 32 Addition operations.</div>
</div>

```markdown
Per Phase (32√ó32 tile):

FLOPs = 32 multiply + 32 add = 64 FLOPs per thread 
Total per phase = 1,024 threads √ó 64 FLOPs = 65,536 FLOPs

Total (8 phases): 

Total FLOPs = 65,536 √ó 8 = 524,288 FLOPs 
Data loaded = 64 KB (32 KB A + 32 KB B across all phases)
Arithmetic Intensity = 524,288 √∑ 65,536 = 8.0 FLOP/byte
```

**Summary:**

| **GEMM Type** | **Arithmetic Intensity** | **Memory Efficiency** | **Kernel Type** |
| --- | --- | --- | --- |
| **Naive GEMM** | 0.25 FLOP/byte | ~ 2% | Memory Bound |
| **Tile GEMM** | 8.0 FLOP/byte | ~ 82% | Near compute-bound |

In conclusion, the transition from naive GEMM to Tiled GEMM represents a significant optimization in matrix multiplication on GPUs, particularly within the constraints of CUDA programming and the Ampere architecture. By leveraging shared memory and a tiling strategy, Tiled GEMM effectively addresses the limitations of data reuse and cache evictions inherent in naive GEMM. This approach maximizes computational efficiency and memory usage, achieving a near compute-bound performance. The careful selection of tile sizes and thread block configurations ensures optimal utilization of the GPU's resources, leading to substantial improvements in arithmetic intensity and overall performance. As a result, Tiled GEMM stands as a powerful technique for enhancing the efficiency of large-scale matrix operations in high-performance computing environments.